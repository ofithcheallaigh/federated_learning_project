\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{parskip}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Federated Learning Literature Review}


\author{
\IEEEauthorblockN{Seán Ó Fithcheallaigh}
\IEEEauthorblockA{\textit{Department of Computing} \\
\textit{Ulster University}\\
Belfast, N. Ireland \\
sofithcheallaigh@gmail.com or o\_fithcheallaigh-s@ulster.ac.uk}
}

\maketitle

\begin{abstract}
This paper presents a literature review on Federated Learning (FL).

\noindent These notes are for personal use.
\end{abstract}

\begin{IEEEkeywords}

\end{IEEEkeywords}

\section{Introduction}
Federated learning is a relatively new topic in artificial intelligence, and it is closely connected with the emerging field of embedded machine learning, also known as TinyML or EmbeddedML. Embedded machine learning has become an essential part of the technology domain given the ubiquitous nature of embedded systems and the IoT and the emerging potential to carry out machine learning tasks at the edge (i.e., in the IoT system or device itself), thereby shifting the implementation of models from high-end systems to low-end clients \cite{b1}. \\
Federated learning (FL) is a topic that entails training statistical models for remote devices or data centres. A good example of a remote device is a mobile phone - there is a wide range of mobile phone hardware configurations available, and they cover a massive network. From this simple example, we can see that training remote devices in a FL setting means that the heterogeneous nature of the hardware, as well as a potentially wide geographical area, are just some of the challenges that must be overcome and will require a radical change from the traditional approach to implementing machine learning models \cite{b2}. \\
Initially, we must consider some challenges that need to be overcome when considering a federated learning system. \textbf{Data privacy and security} are always a paramount concern in any system. The architecture of a federated learning system means that the collected by the remote device will stay on that device, which reduces the risk of data breaches. However, data privacy concerns are not entirely removed, so schemes need to be implemented to ensure that any data is secure. Since FL involves frequent communication between the central server and the remote devices, another concern is \textbf{communication and bandwidth}. Communication overhead can be a substantial part of the battery life for a resource-constrained device. So, optimising the communication protocols is critical to implementing a useful FL network. As mentioned above, \textbf{heterogeneous hardware} will require the development of models that can allow for this heterogeneous nature, which is very important. Depending on the nature of the system, \textbf{data heterogeneity}, where different end devices are collecting different types of data, has the potential to cause problems for the models being implemented, so it is critical to develop a model that will not be hindered by the various types of data and will allow convergence. Another large challenge is \textbf{model aggregation and model updates}, which combines updates from multiple devices. This will require a strategy to maintain the model's performance and accuracy. Creating this balance between local, on-device learning, and global learning will be extremely important. Associated with this is the need for designing a plan for \textbf{strategy for device participation}. By this, we mean designing a plan to select which devices will take part in each round of training while preventing others or preventing devices from dominating the training process. Another potential problem could be \textbf{model overfitting}. This can be a problem because, due to the very local nature of the data, there is the potential of overfitting to the noise in that data. Another area for concern is \textbf{convergence and staleness}, which can be an issue because different devices will update the model at other times (this is staleness) - ensuring convergence while dealing with these delays is complex. Given the complexities of dealing with a wide range of devices, which are updating at different times, etc., robustness and fault tolerance must be integrated system-wide. The system must deal with device failures, dropouts and communication errors without significantly affecting the training process. There are also other concerns around \textbf{regulatory and compliance issues}, such as making sure data usage complies with the laws of the region in which the system is being used. \\
Generally, it can be said that the development of federated learning differs significantly from the more traditional distributed environments, that fundamental advances in areas such as privacy, large-scale machine learning/artificial intelligence tasks, and the need to optimise the networks a federated learning system would use raise new challenges across several fields. \\
This paper aims to present more detail on federated learning by looking at the current state of the art. There will also be a more detailed examination of the challenges faced when trying to develop an efficient and robust system, as well as examining the future directions for federated learning. 

\section{What has gone before}
The federated learning idea has gained ground in industry because it offers several advantages over traditional machine learning approaches. The concept of FL was first proposed by H. Brendan McMahon et al. at Google. This work discusses the ideal types of problems for federated learning. The authors say that the potential problems should have the following properties \cite{b4}:
\begin{itemize}
    \item Training on real-world data from mobile devices provides a distinct advantage over using representative data typically available at a data centre.
    \item The data required is sensitive to privacy issues or is very large, making accessing it via a data centre not the preferred method
    \item For supervised tasks, labels on the data can be inferred naturally from user interactions
\end{itemize}

An often-cited example of a good FL problem is predicting the next word when writing a message on a mobile phone, which falls under the heading of language models. The dataset associated with this problem will likely contain personal messages sent to friends and family, information on a person's financial affairs, information on websites visited, and passwords entered into the device. The dataset would also be massive in scale, with the Google Play Store listing the GBoard application as having been downloaded more than 5 billion times \footnote{https://play.google.com/store/search?q=GBoard\&c=apps} When considering this dataset, three things are apparent. First, this is not the type of dataset that could be easily represented with a proxy dataset \footnote{How a person uses language in information communications is generally different to the language one would get in standard language corpora.}; second, the data is of such a private nature that it would be very likely the owner of the data would not want it to be transmitted from the device to a central server to be used in further training of a global model; third, the text entered into the device is self-labelled for leaning in a language model.

% First, as has been discussed above, there is the ability to allow organisations to train machine learning models on data that cannot be shared because of privacy concerns. Examples would be healthcare data or financial data. Second, FL also allows developers and organisations to use data from multiple sources to improve the accuracy of their models, and third, FL will enable companies to reduce the cost and complexity of storing and managing large amounts of data \cite{b3}.


FL has been used in companies like Microsoft, Google and Apple. Good has used FL to improve the performance of their keyboard application Gboard

% \section*{References}

\begin{thebibliography}{00}
\bibitem{b1} P. Pratim Ray, "A review on TinyML: State-of-the-art and prospects," in Journal of King Saud University –
Computer and Information Sciences, vol. 34, pp. 1595-1623, 2022, doi: 10.1016/j.jksuci.2021.11.019.
\bibitem{b2} T. Li, A. K. Sahu, A. Talwalkar and V. Smith, "Federated Learning: Challenges, Methods, and Future Directions," in IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50-60, May 2020, doi: 10.1109/MSP.2020.2975749.
\bibitem{b4} Guendouzi, B.S., Ouchani, S., EL Assaad, H., EL Zaher, M., A systematic
review of federated learning: Challenges, aggregation methods, and development tools, Journal of
Network and Computer Applications (2023), doi: https://doi.org/10.1016/j.jnca.2023.103714.
\bibitem{b3} B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas, ‘Communication-Efficient Learning of Deep Networks from Decentralized Data’, in Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 20--22 Apr 2017, vol. 54, pp. 1273–1282.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
 %\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
 %\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}


\end{document}
