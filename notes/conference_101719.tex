\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{parskip}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Federated Learning Literature Review}


\author{
\IEEEauthorblockN{Seán Ó Fithcheallaigh}
\IEEEauthorblockA{\textit{Department of Computing} \\
\textit{Ulster University}\\
Belfast, N. Ireland \\
sofithcheallaigh@gmail.com or o\_fithcheallaigh-s@ulster.ac.uk}
}

\maketitle

\begin{abstract}
This paper presents a literature review on Federated Learning (FL).

\noindent These notes are for personal use.
\end{abstract}

\begin{IEEEkeywords}

\end{IEEEkeywords}

\section{Introduction}
Federated learning is a relatively new topic in artificial intelligence, and it is closely connected with the emerging field of embedded machine learning, also known as TinyML or EmbeddedML. Embedded machine learning has become an essential part of the technology domain given the ubiquitous nature of embedded systems and the IoT and the emerging potential to carry out machine learning tasks at the edge (i.e., in the IoT system or device itself), thereby shifting the implementation of models from high-end systems to low-end clients \cite{b1}.

Federated learning (FL) is a topic that entails training statistical models for remote devices or data centres. A good example of a remote device is a mobile phone - there is a wide range of mobile phone hardware configurations available, and they cover a massive network. From this simple example, we can see that training remote devices in a FL setting means that the heterogeneous nature of the hardware, as well as a potentially wide geographical area, are just some of the challenges that must be overcome and will require a radical change from the traditional approach to implementing machine learning models \cite{b2}.

Initially, we must consider some challenges that need to be overcome when considering a federated learning system. \textbf{Data privacy and security} are always a paramount concern in any system. The architecture of a federated learning system means that the collected by the remote device will stay on that device, which reduces the risk of data breaches. However, data privacy concerns are not entirely removed, so schemes need to be implemented to ensure that any data is secure. Since FL involves frequent communication between the central server and the remote devices, another concern is \textbf{communication and bandwidth}. Communication overhead can be a substantial part of the battery life for a resource-constrained device. So, the need to optimise the communication protocols is critical to the implementation of a useful FL network. As mentioned above, \textbf{heterogeneous hardware} will require the development of models that can allow for this heterogeneous nature, which is very important. Depending on the nature of the system, \textbf{data heterogeneity}, where different end devices are collecting different types of data, has the potential to cause problems for the models being implemented, so it is critical to develop a model that will not be hindered by the various types of data and will allow convergence. Another large challenge is \textbf{model aggregation and model updates}, which combines updates from multiple devices. This will require a strategy that will maintain the model's performance and accuracy. Creating this balance between local, on-device learning, and global learning will be extremely important. Associated with this is the need for designing a plan for \textbf{strategy for device participation}. By this, we mean, designing a plan to select which devices will take part in each round of training while preventing others or preventing devices from dominating the training process. Another potential problem could be \textbf{model overfitting}. This can be a problem because, due to the very local nature of the data, there is the potential of overfitting to the noise in that data. Another area for concern is \textbf{convergence and staleness}, which can be an issue because different devices will update the model at different times (this is staleness) - ensuring convergence while dealing with these delays is complex. Given the complexities of dealing with a wide range of devices, which are updating at different times, etc., robustness and fault tolerance must be integrated system-wide. The system will need to be able to deal with device failures, dropouts and communication errors without significantly affecting the overall training process. There are also other concerns around \textbf{regulatory and compliance issues}, such as making sure any usage of data complies with the laws of the region in which the system is being used.

Generally, it can be said that the development of federated learning differs significantly from the more traditional distributed environments, that fundamental advances in areas such as privacy, large-scale machine learning/artificial intelligence tasks, and the need to optimise the networks a federated learning system would use raise new challenges across several fields. 
% \section*{References}

\begin{thebibliography}{00}
\bibitem{b1} P. Pratim Ray, "A review on TinyML: State-of-the-art and prospects," in Journal of King Saud University –
Computer and Information Sciences, vol. 34, pp. 1595-1623, 2022, doi: 10.1016/j.jksuci.2021.11.019.

\bibitem{b2} T. Li, A. K. Sahu, A. Talwalkar and V. Smith, "Federated Learning: Challenges, Methods, and Future Directions," in IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50-60, May 2020, doi: 10.1109/MSP.2020.2975749.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
 %\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
 %\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}


\end{document}
